{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "2oh8AaSz5-lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A Decision Tree is a popular supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it is a model that predicts the class label of an instance by learning decision rules inferred from the features of the data.\n",
        "\n",
        "How it Works for Classification\n",
        "\n",
        "The process of building a decision tree for classification can be summarized as:\n",
        "\n",
        "- Select the Best Feature to Split:\n",
        "\n",
        "The algorithm chooses the feature that best separates the classes using a splitting criterion such as:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Entropy / Information Gain\n",
        "\n",
        "Chi-square\n",
        "\n",
        "- Split the Dataset:\n",
        "\n",
        "The dataset is divided into subsets based on the values of the selected feature.\n",
        "\n",
        "- Repeat Recursively (Recursive Partitioning):\n",
        "\n",
        "For each subset, steps 1 and 2 are repeated until:\n",
        "\n",
        "All samples in a node belong to the same class, or\n",
        "\n",
        "A stopping criterion is reached (like maximum tree depth or minimum samples per leaf).\n",
        "\n",
        "- Assign Class Labels:\n",
        "\n",
        "Each leaf node is assigned the class label that is most frequent among the samples in that node."
      ],
      "metadata": {
        "id": "pTNC9QiQ6Aah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- In decision trees, we need a way to decide which feature to split on at each node. This is done using impurity measures that quantify how “mixed” the classes are in a dataset. The two most common measures are Gini Impurity and Entropy.\n",
        "\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element from the dataset if we randomly assign a label according to the class distribution in that node.\n",
        "\n",
        "How They Impact Splits in a Decision Tree\n",
        "\n",
        "- Goal: Choose the split that reduces impurity the most (makes child nodes as “pure” as possible).\n",
        "\n",
        "- Gini and Entropy are used as criteria for evaluating splits:\n",
        "\n",
        "- Gini Impurity → tries to minimize the probability of misclassification\n",
        "\n",
        "- Entropy → tries to maximize information gain (reduction in disorder)\n",
        "\n",
        "- Both often lead to similar splits, but Gini is slightly faster to compute, so it is often used in libraries like scikit-learn."
      ],
      "metadata": {
        "id": "2PO0eOIZ63Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "- | Aspect                | Pre-Pruning                  | Post-Pruning                        |\n",
        "| --------------------- | ---------------------------- | ----------------------------------- |\n",
        "| **When Applied**      | During tree construction     | After full tree is built            |\n",
        "| **Overfitting**       | Prevented early              | Reduced after analysis              |\n",
        "| **Computation**       | Faster (smaller tree)        | Slower (build full tree first)      |\n",
        "| **Accuracy**          | Might underfit if too strict | Usually more accurate if tuned well |\n",
        "| **Example Parameter** | max_depth, min_samples_split | CCP (α), reduced error pruning      |\n"
      ],
      "metadata": {
        "id": "104Zq6Ap7ejm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "- Information Gain measures the reduction in uncertainty (entropy) about the target variable after splitting the dataset based on a feature. In other words, it tells us how much “information” a feature gives us about the class labels. The feature with the highest information gain is chosen for the split.\n",
        "\n",
        "Information Gain is Important because-\n",
        "\n",
        "- Helps select the feature that best separates the classes.\n",
        "\n",
        "- Reduces uncertainty most efficiently, leading to smaller, more accurate trees.\n",
        "\n",
        "- Ensures the tree splits on features that provide the most predictive power."
      ],
      "metadata": {
        "id": "jL86eaXD7ySH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- Real-World Applications of Decision Trees\n",
        "A. Classification Tasks\n",
        "\n",
        "Healthcare Diagnosis\n",
        "\n",
        "Predicting whether a patient has a disease based on symptoms and test results.\n",
        "\n",
        "Example: Classifying whether a tumor is malignant or benign.\n",
        "\n",
        "Credit Risk Assessment\n",
        "\n",
        "Banks use decision trees to classify loan applicants as “low risk” or “high risk” based on financial history, income, and credit score.\n",
        "\n",
        "Customer Segmentation\n",
        "\n",
        "E-commerce sites classify customers into categories (e.g., likely to buy, occasional buyers, or churn risk).\n",
        "\n",
        "Fraud Detection\n",
        "\n",
        "Classifying financial transactions as “fraudulent” or “legitimate” based on transaction patterns.\n",
        "\n",
        "B. Regression Tasks\n",
        "\n",
        "Predicting House Prices\n",
        "\n",
        "Using features like area, location, and number of bedrooms.\n",
        "\n",
        "Sales Forecasting\n",
        "\n",
        "Predicting future sales of products based on historical data and seasonality.\n",
        "\n",
        "C. Other Applications\n",
        "\n",
        "Decision Support Systems\n",
        "\n",
        "Help managers make decisions in business or logistics.\n",
        "\n",
        "Game AI\n",
        "\n",
        "Simple AI in games uses decision trees for strategy or move selection.\n",
        "\n",
        "Manufacturing\n",
        "\n",
        "Predicting machine failure or quality issues based on sensor data.\n",
        "\n",
        "| Advantage                            | Explanation                                                               |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------- |\n",
        "| **Easy to Understand**               | Trees can be visualized; even non-experts can interpret them.             |\n",
        "| **No Need for Feature Scaling**      | Works well with both numerical and categorical data.                      |\n",
        "| **Handles Non-linear Relationships** | Can model complex relationships between features and target.              |\n",
        "| **Automatic Feature Selection**      | Splitting naturally chooses the most informative features.                |\n",
        "| **Fast Prediction**                  | Once trained, predictions are made by simple traversal from root to leaf. |\n",
        "\n",
        "\n",
        "| Limitation                                 | Explanation                                                                                      |\n",
        "| ------------------------------------------ | ------------------------------------------------------------------------------------------------ |\n",
        "| **Prone to Overfitting**                   | Can create very deep trees that fit noise in the training data.                                  |\n",
        "| **Unstable**                               | Small changes in data can lead to very different trees.                                          |\n",
        "| **Less Accurate than Ensembles**           | Often outperformed by Random Forests or Gradient Boosting.                                       |\n",
        "| **Bias Towards Features with More Levels** | Features with many categories may be chosen for splits unfairly.                                 |\n",
        "| **Greedy Splitting**                       | The algorithm makes local decisions (best split at each node) which may not be globally optimal. |\n"
      ],
      "metadata": {
        "id": "zpLrro3A8RXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "x07952Jj80BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "9B3VDFuB89pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "F01zhAzV9Svl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_depth3:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n",
        "\n",
        "Output:\n",
        "Accuracy with max_depth=3: 0.94\n",
        "Accuracy with fully-grown tree: 1.00\n"
      ],
      "metadata": {
        "id": "JeH5cngb9dsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "E_MTLVFO90Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "feature_names = boston.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "importances = regressor.feature_importances_\n",
        "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feat_imp)\n",
        "\n",
        "Outout:\n",
        "Mean Squared Error (MSE): 10.93\n"
      ],
      "metadata": {
        "id": "ftMcmWUv95F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "CfoXK_v8-Myf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dtree,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model: {accuracy:.2f}\")\n",
        "\n",
        "Output:\n",
        "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n"
      ],
      "metadata": {
        "id": "lEPDET-A-WsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "0OnRycU0-sSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                           param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "Vh9AqxKb-xLx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}